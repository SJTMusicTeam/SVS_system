"""Copyright [2020] [Jiatong Shi].

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""
from collections import OrderedDict
import copy
import math
import numpy as np
from scipy.constants import hp
import torch as t
from torch.autograd import Variable
import torch.nn as nn

# import torch.nn.functional as F


def clones(module, N):
    """clones."""
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


class Linear(nn.Module):
    """Linear Module."""

    def __init__(self, in_dim, out_dim, bias=True, w_init="linear"):
        """init."""
        # :param in_dim: dimension of input
        # :param out_dim: dimension of output
        # :param bias: boolean. if True, bias is included.
        # :param w_init: str. weight inits with xavier initialization.

        super(Linear, self).__init__()
        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)

        nn.init.xavier_uniform_(
            self.linear_layer.weight, gain=nn.init.calculate_gain(w_init)
        )

    def forward(self, x):
        """forward."""
        return self.linear_layer(x)


class Conv(nn.Module):
    """Convolution Module."""

    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size=1,
        stride=1,
        padding=0,
        dilation=1,
        bias=True,
        w_init="linear",
    ):
        """init."""
        # :param in_channels: dimension of input
        # :param out_channels: dimension of output
        # :param kernel_size: size of kernel
        # :param stride: size of stride
        # :param padding: size of padding
        # :param dilation: dilation rate
        # :param bias: boolean. if True, bias is included.
        # :param w_init: str. weight inits with xavier initialization.

        super(Conv, self).__init__()

        self.conv = nn.Conv1d(
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            bias=bias,
        )

        nn.init.xavier_uniform_(self.conv.weight, gain=nn.init.calculate_gain(w_init))

    def forward(self, x):
        """forward."""
        x = self.conv(x)
        return x


class EncoderPrenet(nn.Module):
    """Pre-network for Encoder consists of convolution networks."""

    def __init__(self, embedding_size, num_hidden):
        """init."""
        super(EncoderPrenet, self).__init__()
        self.embedding_size = embedding_size
        # self.embed=nn.Embedding(len(symbols), embedding_size, padding_idx=0)

        self.conv1 = Conv(
            in_channels=embedding_size,
            out_channels=num_hidden,
            kernel_size=5,
            padding=int(np.floor(5 / 2)),
            w_init="relu",
        )
        self.conv2 = Conv(
            in_channels=num_hidden,
            out_channels=num_hidden,
            kernel_size=5,
            padding=int(np.floor(5 / 2)),
            w_init="relu",
        )

        self.conv3 = Conv(
            in_channels=num_hidden,
            out_channels=num_hidden,
            kernel_size=5,
            padding=int(np.floor(5 / 2)),
            w_init="relu",
        )

        self.batch_norm1 = nn.BatchNorm1d(num_hidden)
        self.batch_norm2 = nn.BatchNorm1d(num_hidden)
        self.batch_norm3 = nn.BatchNorm1d(num_hidden)

        self.dropout1 = nn.Dropout(p=0.2)
        self.dropout2 = nn.Dropout(p=0.2)
        self.dropout3 = nn.Dropout(p=0.2)
        self.projection = Linear(num_hidden, num_hidden)

    def forward(self, input_):
        """forward."""
        input_ = self.embed(input_)
        input_ = input_.transpose(1, 2)
        input_ = self.dropout1(t.relu(self.batch_norm1(self.conv1(input_))))
        input_ = self.dropout2(t.relu(self.batch_norm2(self.conv2(input_))))
        input_ = self.dropout3(t.relu(self.batch_norm3(self.conv3(input_))))
        input_ = input_.transpose(1, 2)
        input_ = self.projection(input_)

        return input_


class FFN(nn.Module):
    """Positionwise Feed-Forward Network."""

    def __init__(self, num_hidden):
        """:param num_hidden: dimension of hidden."""
        super(FFN, self).__init__()
        self.w_1 = Conv(num_hidden, num_hidden * 4, kernel_size=1, w_init="relu")
        self.w_2 = Conv(num_hidden * 4, num_hidden, kernel_size=1)
        self.dropout = nn.Dropout(p=0.1)
        self.layer_norm = nn.LayerNorm(num_hidden)

    def forward(self, input_):
        """forward."""
        # FFN Network
        x = input_.transpose(1, 2)
        x = self.w_2(t.relu(self.w_1(x)))
        x = x.transpose(1, 2)

        # residual connection
        x = x + input_

        # dropout
        # x = self.dropout(x)

        # layer normalization
        x = self.layer_norm(x)

        return x


class PostConvNet(nn.Module):
    """Post Convolutional Network (mel --> mel)."""

    def __init__(self, num_hidden):
        """:param num_hidden: dimension of hidden."""
        super(PostConvNet, self).__init__()
        self.conv1 = Conv(
            in_channels=hp.num_mels * hp.outputs_per_step,
            out_channels=num_hidden,
            kernel_size=5,
            padding=4,
            w_init="tanh",
        )
        self.conv_list = clones(
            Conv(
                in_channels=num_hidden,
                out_channels=num_hidden,
                kernel_size=5,
                padding=4,
                w_init="tanh",
            ),
            3,
        )
        self.conv2 = Conv(
            in_channels=num_hidden,
            out_channels=hp.num_mels * hp.outputs_per_step,
            kernel_size=5,
            padding=4,
        )

        self.batch_norm_list = clones(nn.BatchNorm1d(num_hidden), 3)
        self.pre_batchnorm = nn.BatchNorm1d(num_hidden)

        self.dropout1 = nn.Dropout(p=0.1)
        self.dropout_list = nn.ModuleList([nn.Dropout(p=0.1) for _ in range(3)])

    def forward(self, input_, mask=None):
        """forward."""
        # Causal Convolution (for auto-regressive)
        input_ = self.dropout1(
            t.tanh(self.pre_batchnorm(self.conv1(input_)[:, :, :-4]))
        )
        for batch_norm, conv, dropout in zip(
            self.batch_norm_list, self.conv_list, self.dropout_list
        ):
            input_ = dropout(t.tanh(batch_norm(conv(input_)[:, :, :-4])))
        input_ = self.conv2(input_)[:, :, :-4]
        return input_


class MultiheadAttention(nn.Module):
    """Multihead attention mechanism (dot attention)."""

    def __init__(self, num_hidden_k):
        """:param num_hidden_k: dimension of hidden."""
        super(MultiheadAttention, self).__init__()

        self.num_hidden_k = num_hidden_k
        self.attn_dropout = nn.Dropout(p=0.1)

    def forward(
        self,
        key,
        value,
        query,
        mask=None,
        query_mask=None,
        gaussian_factor=None,
    ):
        """forward."""
        # Get attention score
        attn = t.bmm(query, key.transpose(1, 2))
        attn = attn / math.sqrt(self.num_hidden_k)
        if gaussian_factor is not None:
            attn = attn - gaussian_factor

        # Masking to ignore padding (key side)
        if mask is not None:
            attn = attn.masked_fill(mask, -(2 ** 32) + 1)
            attn = t.softmax(attn, dim=-1)
        else:
            attn = t.softmax(attn, dim=-1)

        # Masking to ignore padding (query side)
        if query_mask is not None:
            attn = attn * query_mask

        # Dropout
        # attn = self.attn_dropout(attn)

        # Get Context Vector
        result = t.bmm(attn, value)

        return result, attn


class Attention(nn.Module):
    """Attention Network."""

    def __init__(self, num_hidden, h=4, local_gaussian=False):
        """init."""
        # :param num_hidden: dimension of hidden
        # :param h: num of heads

        super(Attention, self).__init__()

        self.num_hidden = num_hidden
        self.num_hidden_per_attn = num_hidden // h
        self.h = h

        self.key = Linear(num_hidden, num_hidden, bias=False)
        self.value = Linear(num_hidden, num_hidden, bias=False)
        self.query = Linear(num_hidden, num_hidden, bias=False)

        self.multihead = MultiheadAttention(self.num_hidden_per_attn)

        self.local_gaussian = local_gaussian
        if local_gaussian:
            self.local_gaussian_factor = Variable(
                t.tensor(30.0), requires_grad=True
            ).float()
        else:
            self.local_gaussian_factor = None

        self.residual_dropout = nn.Dropout(p=0.1)

        self.final_linear = Linear(num_hidden * 2, num_hidden)

        self.layer_norm_1 = nn.LayerNorm(num_hidden)

    def forward(self, memory, decoder_input, mask=None, query_mask=None):
        """forward."""
        batch_size = memory.size(0)
        seq_k = memory.size(1)
        seq_q = decoder_input.size(1)

        # Repeat masks h times
        if query_mask is not None:
            query_mask = query_mask.unsqueeze(-1).repeat(1, 1, seq_k)
            query_mask = query_mask.repeat(self.h, 1, 1)
        if mask is not None:
            mask = mask.repeat(self.h, 1, 1)

        # Make multihead
        key = self.key(memory).view(batch_size, seq_k, self.h, self.num_hidden_per_attn)
        value = self.value(memory).view(
            batch_size, seq_k, self.h, self.num_hidden_per_attn
        )
        query = self.query(decoder_input).view(
            batch_size, seq_q, self.h, self.num_hidden_per_attn
        )

        key = (
            key.permute(2, 0, 1, 3)
            .contiguous()
            .view(-1, seq_k, self.num_hidden_per_attn)
        )
        value = (
            value.permute(2, 0, 1, 3)
            .contiguous()
            .view(-1, seq_k, self.num_hidden_per_attn)
        )
        query = (
            query.permute(2, 0, 1, 3)
            .contiguous()
            .view(-1, seq_q, self.num_hidden_per_attn)
        )

        # add gaussian or not
        if self.local_gaussian:
            row = (
                t.arange(1, seq_k + 1)
                .unsqueeze(0)
                .unsqueeze(-1)
                .repeat(batch_size * self.h, 1, seq_k)
            )
            col = (
                t.arange(1, seq_k + 1)
                .unsqueeze(0)
                .unsqueeze(0)
                .repeat(batch_size * self.h, seq_k, 1)
            )
            local_gaussian = t.pow(row - col, 2).float().to(key.device.type)
            self.local_gaussian_factor = self.local_gaussian_factor.to(key.device.type)
            local_gaussian = local_gaussian / self.local_gaussian_factor
        else:
            local_gaussian = None

        # Get context vector
        result, attns = self.multihead(
            key,
            value,
            query,
            mask=mask,
            query_mask=query_mask,
            gaussian_factor=local_gaussian,
        )

        attns = attns.view(self.h, batch_size, seq_q, seq_k)
        attns = attns.permute(1, 0, 2, 3)

        # Concatenate all multihead context vector
        result = result.view(self.h, batch_size, seq_q, self.num_hidden_per_attn)
        result = result.permute(1, 2, 0, 3).contiguous().view(batch_size, seq_q, -1)

        # Concatenate context vector with input (most important)
        result = t.cat([decoder_input, result], dim=-1)

        # Final linear
        result = self.final_linear(result)

        # Residual dropout & connection
        result = result + decoder_input

        # result = self.residual_dropout(result)

        # Layer normalization
        result = self.layer_norm_1(result)

        return result, attns


class Prenet(nn.Module):
    """Prenet before passing through the network."""

    def __init__(self, input_size, hidden_size, output_size, p=0.5):
        # :param input_size: dimension of input
        # :param hidden_size: dimension of hidden unit
        # :param output_size: dimension of output
        """init."""
        super(Prenet, self).__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.layer = nn.Sequential(
            OrderedDict(
                [
                    ("fc1", Linear(self.input_size, self.hidden_size)),
                    ("relu1", nn.ReLU()),
                    ("dropout1", nn.Dropout(p)),
                    ("fc2", Linear(self.hidden_size, self.output_size)),
                    ("relu2", nn.ReLU()),
                    ("dropout2", nn.Dropout(p)),
                ]
            )
        )

    def forward(self, input_):
        """forward."""
        out = self.layer(input_)

        return out


class CBHG(nn.Module):
    """CBHG Module."""

    def __init__(
        self,
        hidden_size,
        K=16,
        projection_size=256,
        num_gru_layers=2,
        max_pool_kernel_size=2,
        is_post=False,
    ):
        """init."""
        # :param hidden_size: dimension of hidden unit
        # :param K: # of convolution banks
        # :param projection_size: dimension of projection unit
        # :param num_gru_layers: # of layers of GRUcell
        # :param max_pool_kernel_size: max pooling kernel size
        # :param is_post: whether post processing or not

        super(CBHG, self).__init__()
        self.hidden_size = hidden_size
        self.projection_size = projection_size
        self.convbank_list = nn.ModuleList()
        self.convbank_list.append(
            nn.Conv1d(
                in_channels=projection_size,
                out_channels=hidden_size,
                kernel_size=1,
                padding=int(np.floor(1 / 2)),
            )
        )

        for i in range(2, K + 1):
            self.convbank_list.append(
                nn.Conv1d(
                    in_channels=hidden_size,
                    out_channels=hidden_size,
                    kernel_size=i,
                    padding=int(np.floor(i / 2)),
                )
            )

        self.batchnorm_list = nn.ModuleList()
        for i in range(1, K + 1):
            self.batchnorm_list.append(nn.BatchNorm1d(hidden_size))

        convbank_outdim = hidden_size * K

        self.conv_projection_1 = nn.Conv1d(
            in_channels=convbank_outdim,
            out_channels=hidden_size,
            kernel_size=3,
            padding=int(np.floor(3 / 2)),
        )
        self.conv_projection_2 = nn.Conv1d(
            in_channels=hidden_size,
            out_channels=projection_size,
            kernel_size=3,
            padding=int(np.floor(3 / 2)),
        )
        self.batchnorm_proj_1 = nn.BatchNorm1d(hidden_size)

        self.batchnorm_proj_2 = nn.BatchNorm1d(projection_size)

        self.max_pool = nn.MaxPool1d(max_pool_kernel_size, stride=1, padding=1)
        self.highway = Highwaynet(self.projection_size)
        self.gru = nn.GRU(
            self.projection_size,
            self.hidden_size // 2,
            num_layers=num_gru_layers,
            batch_first=True,
            bidirectional=True,
        )

    def _conv_fit_dim(self, x, kernel_size=3):
        """_conv_fit_dim."""
        if kernel_size % 2 == 0:
            return x[:, :, :-1]
        else:
            return x

    def forward(self, input_):
        """forward."""
        input_ = input_.contiguous()
        # batch_size = input_.size(0)
        # total_length = input_.size(-1)

        convbank_list = list()
        convbank_input = input_

        # Convolution bank filters
        for k, (conv, batchnorm) in enumerate(
            zip(self.convbank_list, self.batchnorm_list)
        ):
            convbank_input = t.relu(
                batchnorm(self._conv_fit_dim(conv(convbank_input), k + 1).contiguous())
            )
            convbank_list.append(convbank_input)

        # Concatenate all features
        conv_cat = t.cat(convbank_list, dim=1)

        # Max pooling
        conv_cat = self.max_pool(conv_cat)[:, :, :-1]

        # Projection
        conv_projection = t.relu(
            self.batchnorm_proj_1(self._conv_fit_dim(self.conv_projection_1(conv_cat)))
        )
        conv_projection = (
            self.batchnorm_proj_2(
                self._conv_fit_dim(self.conv_projection_2(conv_projection))
            )
            + input_
        )

        # Highway networks
        highway = self.highway.forward(conv_projection.transpose(1, 2))

        # Bidirectional GRU

        self.gru.flatten_parameters()
        out, _ = self.gru(highway)

        return out


class Highwaynet(nn.Module):
    """Highway network."""

    def __init__(self, num_units, num_layers=4):
        """init."""
        # :param num_units: dimension of hidden unit
        # :param num_layers: # of highway layers

        super(Highwaynet, self).__init__()
        self.num_units = num_units
        self.num_layers = num_layers
        self.gates = nn.ModuleList()
        self.linears = nn.ModuleList()
        for _ in range(self.num_layers):
            self.linears.append(Linear(num_units, num_units))
            self.gates.append(Linear(num_units, num_units))

    def forward(self, input_):
        """forward."""
        out = input_

        # highway gated function
        for fc1, fc2 in zip(self.linears, self.gates):

            h = t.relu(fc1.forward(out))
            t_ = t.sigmoid(fc2.forward(out))

            c = 1.0 - t_
            out = h * t_ + out * c

        return out
